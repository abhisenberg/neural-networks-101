[
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "optim",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "optim",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "optim",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "optim",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "datasets",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "datasets",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "datasets",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "datasets",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "models",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "dataset",
        "kind": 5,
        "importPath": "001-basic-nn.01-mnist",
        "description": "001-basic-nn.01-mnist",
        "peekOfCode": "dataset = \"F_MNIST_data\"\ntrained_dataset_name = \"f_mnist_model.pth\"\n# step 1: load the data, normalize it\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),])\ntrainset =  datasets.MNIST('~/pytorch/'+dataset+'/', download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# step 2: define the model, the criterion (error function), optim (gradient function)\nmodel = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),",
        "detail": "001-basic-nn.01-mnist",
        "documentation": {}
    },
    {
        "label": "trained_dataset_name",
        "kind": 5,
        "importPath": "001-basic-nn.01-mnist",
        "description": "001-basic-nn.01-mnist",
        "peekOfCode": "trained_dataset_name = \"f_mnist_model.pth\"\n# step 1: load the data, normalize it\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),])\ntrainset =  datasets.MNIST('~/pytorch/'+dataset+'/', download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# step 2: define the model, the criterion (error function), optim (gradient function)\nmodel = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 128),",
        "detail": "001-basic-nn.01-mnist",
        "documentation": {}
    },
    {
        "label": "transform",
        "kind": 5,
        "importPath": "001-basic-nn.01-mnist",
        "description": "001-basic-nn.01-mnist",
        "peekOfCode": "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),])\ntrainset =  datasets.MNIST('~/pytorch/'+dataset+'/', download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# step 2: define the model, the criterion (error function), optim (gradient function)\nmodel = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 128),\n    nn.ReLU(),\n    nn.Linear(128, 64),",
        "detail": "001-basic-nn.01-mnist",
        "documentation": {}
    },
    {
        "label": "trainset",
        "kind": 5,
        "importPath": "001-basic-nn.01-mnist",
        "description": "001-basic-nn.01-mnist",
        "peekOfCode": "trainset =  datasets.MNIST('~/pytorch/'+dataset+'/', download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# step 2: define the model, the criterion (error function), optim (gradient function)\nmodel = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 128),\n    nn.ReLU(),\n    nn.Linear(128, 64),\n    nn.ReLU(),",
        "detail": "001-basic-nn.01-mnist",
        "documentation": {}
    },
    {
        "label": "trainloader",
        "kind": 5,
        "importPath": "001-basic-nn.01-mnist",
        "description": "001-basic-nn.01-mnist",
        "peekOfCode": "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# step 2: define the model, the criterion (error function), optim (gradient function)\nmodel = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 128),\n    nn.ReLU(),\n    nn.Linear(128, 64),\n    nn.ReLU(),\n    nn.Linear(64, 10),",
        "detail": "001-basic-nn.01-mnist",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "001-basic-nn.01-mnist",
        "description": "001-basic-nn.01-mnist",
        "peekOfCode": "model = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 128),\n    nn.ReLU(),\n    nn.Linear(128, 64),\n    nn.ReLU(),\n    nn.Linear(64, 10),\n    nn.LogSoftmax(dim=1)\n)",
        "detail": "001-basic-nn.01-mnist",
        "documentation": {}
    },
    {
        "label": "criterion",
        "kind": 5,
        "importPath": "001-basic-nn.01-mnist",
        "description": "001-basic-nn.01-mnist",
        "peekOfCode": "criterion = nn.NLLLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.003)\n# step 3: run the training in loops\nepochs = 5\nfor e in range(epochs):\n    runningloss = 0\n    for images, labels in trainloader:\n        optimizer.zero_grad()   #3a. clear the older grad values, bcs they accumulate\n        images = images.view(images.shape[0], -1)   #3b. flatten the images\n        output = model.forward(images)      #3c. forward pass to get the output",
        "detail": "001-basic-nn.01-mnist",
        "documentation": {}
    },
    {
        "label": "optimizer",
        "kind": 5,
        "importPath": "001-basic-nn.01-mnist",
        "description": "001-basic-nn.01-mnist",
        "peekOfCode": "optimizer = optim.SGD(model.parameters(), lr=0.003)\n# step 3: run the training in loops\nepochs = 5\nfor e in range(epochs):\n    runningloss = 0\n    for images, labels in trainloader:\n        optimizer.zero_grad()   #3a. clear the older grad values, bcs they accumulate\n        images = images.view(images.shape[0], -1)   #3b. flatten the images\n        output = model.forward(images)      #3c. forward pass to get the output\n        loss = criterion(output, labels)    #3d. calculate loss from the output",
        "detail": "001-basic-nn.01-mnist",
        "documentation": {}
    },
    {
        "label": "epochs",
        "kind": 5,
        "importPath": "001-basic-nn.01-mnist",
        "description": "001-basic-nn.01-mnist",
        "peekOfCode": "epochs = 5\nfor e in range(epochs):\n    runningloss = 0\n    for images, labels in trainloader:\n        optimizer.zero_grad()   #3a. clear the older grad values, bcs they accumulate\n        images = images.view(images.shape[0], -1)   #3b. flatten the images\n        output = model.forward(images)      #3c. forward pass to get the output\n        loss = criterion(output, labels)    #3d. calculate loss from the output\n        loss.backward()     #3e. run the backward pass, calc the gradient values\n        optimizer.step()    #3f. apply the gradient descent and update the weights",
        "detail": "001-basic-nn.01-mnist",
        "documentation": {}
    },
    {
        "label": "dataset",
        "kind": 5,
        "importPath": "001-basic-nn.02-mnist-eval",
        "description": "001-basic-nn.02-mnist-eval",
        "peekOfCode": "dataset = \"F_MNIST_data\"\ntrained_dataset_name = \"f_mnist_model.pth\"\n# step 1: load the data, normalize it\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),])\ntrainset =  datasets.MNIST('~/pytorch/'+dataset+'/', download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# step 2: create the loaded model, load the training weights\nloaded_model = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),",
        "detail": "001-basic-nn.02-mnist-eval",
        "documentation": {}
    },
    {
        "label": "trained_dataset_name",
        "kind": 5,
        "importPath": "001-basic-nn.02-mnist-eval",
        "description": "001-basic-nn.02-mnist-eval",
        "peekOfCode": "trained_dataset_name = \"f_mnist_model.pth\"\n# step 1: load the data, normalize it\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),])\ntrainset =  datasets.MNIST('~/pytorch/'+dataset+'/', download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# step 2: create the loaded model, load the training weights\nloaded_model = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 128),",
        "detail": "001-basic-nn.02-mnist-eval",
        "documentation": {}
    },
    {
        "label": "transform",
        "kind": 5,
        "importPath": "001-basic-nn.02-mnist-eval",
        "description": "001-basic-nn.02-mnist-eval",
        "peekOfCode": "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),])\ntrainset =  datasets.MNIST('~/pytorch/'+dataset+'/', download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# step 2: create the loaded model, load the training weights\nloaded_model = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 128),\n    nn.ReLU(),\n    nn.Linear(128, 64),",
        "detail": "001-basic-nn.02-mnist-eval",
        "documentation": {}
    },
    {
        "label": "trainset",
        "kind": 5,
        "importPath": "001-basic-nn.02-mnist-eval",
        "description": "001-basic-nn.02-mnist-eval",
        "peekOfCode": "trainset =  datasets.MNIST('~/pytorch/'+dataset+'/', download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# step 2: create the loaded model, load the training weights\nloaded_model = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 128),\n    nn.ReLU(),\n    nn.Linear(128, 64),\n    nn.ReLU(),",
        "detail": "001-basic-nn.02-mnist-eval",
        "documentation": {}
    },
    {
        "label": "trainloader",
        "kind": 5,
        "importPath": "001-basic-nn.02-mnist-eval",
        "description": "001-basic-nn.02-mnist-eval",
        "peekOfCode": "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# step 2: create the loaded model, load the training weights\nloaded_model = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 128),\n    nn.ReLU(),\n    nn.Linear(128, 64),\n    nn.ReLU(),\n    nn.Linear(64, 10),",
        "detail": "001-basic-nn.02-mnist-eval",
        "documentation": {}
    },
    {
        "label": "loaded_model",
        "kind": 5,
        "importPath": "001-basic-nn.02-mnist-eval",
        "description": "001-basic-nn.02-mnist-eval",
        "peekOfCode": "loaded_model = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 128),\n    nn.ReLU(),\n    nn.Linear(128, 64),\n    nn.ReLU(),\n    nn.Linear(64, 10),\n    nn.LogSoftmax(dim=1)\n)",
        "detail": "001-basic-nn.02-mnist-eval",
        "documentation": {}
    },
    {
        "label": "Classifier",
        "kind": 6,
        "importPath": "001-basic-nn.03-fmnist-dropout",
        "description": "001-basic-nn.03-fmnist-dropout",
        "peekOfCode": "class Classifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.fc4 = nn.Linear(64, 10)\n        # Dropout module with 0.2 drop probability\n        self.dropout = nn.Dropout(p=0.2)\n    def forward(self, x):",
        "detail": "001-basic-nn.03-fmnist-dropout",
        "documentation": {}
    },
    {
        "label": "dataset",
        "kind": 5,
        "importPath": "001-basic-nn.03-fmnist-dropout",
        "description": "001-basic-nn.03-fmnist-dropout",
        "peekOfCode": "dataset = \"F_MNIST_data\"\ntrained_dataset_name = \"f_mnist_dropout_model.pth\"\n# Define a transform to normalize the data\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, ), (0.5, ))])\n# Download and load the training data\ntrainset = datasets.FashionMNIST(f\"~/.pytorch/{dataset}/\", download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# Download and load the test data\ntestset = datasets.FashionMNIST(f\"~/.pytorch/{dataset}/\", download=True, train=False, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)",
        "detail": "001-basic-nn.03-fmnist-dropout",
        "documentation": {}
    },
    {
        "label": "trained_dataset_name",
        "kind": 5,
        "importPath": "001-basic-nn.03-fmnist-dropout",
        "description": "001-basic-nn.03-fmnist-dropout",
        "peekOfCode": "trained_dataset_name = \"f_mnist_dropout_model.pth\"\n# Define a transform to normalize the data\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, ), (0.5, ))])\n# Download and load the training data\ntrainset = datasets.FashionMNIST(f\"~/.pytorch/{dataset}/\", download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# Download and load the test data\ntestset = datasets.FashionMNIST(f\"~/.pytorch/{dataset}/\", download=True, train=False, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\nclass Classifier(nn.Module):",
        "detail": "001-basic-nn.03-fmnist-dropout",
        "documentation": {}
    },
    {
        "label": "transform",
        "kind": 5,
        "importPath": "001-basic-nn.03-fmnist-dropout",
        "description": "001-basic-nn.03-fmnist-dropout",
        "peekOfCode": "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, ), (0.5, ))])\n# Download and load the training data\ntrainset = datasets.FashionMNIST(f\"~/.pytorch/{dataset}/\", download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# Download and load the test data\ntestset = datasets.FashionMNIST(f\"~/.pytorch/{dataset}/\", download=True, train=False, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\nclass Classifier(nn.Module):\n    def __init__(self):\n        super().__init__()",
        "detail": "001-basic-nn.03-fmnist-dropout",
        "documentation": {}
    },
    {
        "label": "trainset",
        "kind": 5,
        "importPath": "001-basic-nn.03-fmnist-dropout",
        "description": "001-basic-nn.03-fmnist-dropout",
        "peekOfCode": "trainset = datasets.FashionMNIST(f\"~/.pytorch/{dataset}/\", download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# Download and load the test data\ntestset = datasets.FashionMNIST(f\"~/.pytorch/{dataset}/\", download=True, train=False, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\nclass Classifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 256)\n        self.fc2 = nn.Linear(256, 128)",
        "detail": "001-basic-nn.03-fmnist-dropout",
        "documentation": {}
    },
    {
        "label": "trainloader",
        "kind": 5,
        "importPath": "001-basic-nn.03-fmnist-dropout",
        "description": "001-basic-nn.03-fmnist-dropout",
        "peekOfCode": "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# Download and load the test data\ntestset = datasets.FashionMNIST(f\"~/.pytorch/{dataset}/\", download=True, train=False, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\nclass Classifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 64)",
        "detail": "001-basic-nn.03-fmnist-dropout",
        "documentation": {}
    },
    {
        "label": "testset",
        "kind": 5,
        "importPath": "001-basic-nn.03-fmnist-dropout",
        "description": "001-basic-nn.03-fmnist-dropout",
        "peekOfCode": "testset = datasets.FashionMNIST(f\"~/.pytorch/{dataset}/\", download=True, train=False, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\nclass Classifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.fc4 = nn.Linear(64, 10)\n        # Dropout module with 0.2 drop probability",
        "detail": "001-basic-nn.03-fmnist-dropout",
        "documentation": {}
    },
    {
        "label": "testloader",
        "kind": 5,
        "importPath": "001-basic-nn.03-fmnist-dropout",
        "description": "001-basic-nn.03-fmnist-dropout",
        "peekOfCode": "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\nclass Classifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.fc4 = nn.Linear(64, 10)\n        # Dropout module with 0.2 drop probability\n        self.dropout = nn.Dropout(p=0.2)",
        "detail": "001-basic-nn.03-fmnist-dropout",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "001-basic-nn.03-fmnist-dropout",
        "description": "001-basic-nn.03-fmnist-dropout",
        "peekOfCode": "model = Classifier()\ncriterion = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.003)\nepochs = 30\nsteps = 0\ntrain_losses, test_losses = [], []\nfor e in range(epochs):\n    running_loss = 0\n    model.train()\n    print(f\"Running for epoch {e+1}...\")",
        "detail": "001-basic-nn.03-fmnist-dropout",
        "documentation": {}
    },
    {
        "label": "criterion",
        "kind": 5,
        "importPath": "001-basic-nn.03-fmnist-dropout",
        "description": "001-basic-nn.03-fmnist-dropout",
        "peekOfCode": "criterion = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.003)\nepochs = 30\nsteps = 0\ntrain_losses, test_losses = [], []\nfor e in range(epochs):\n    running_loss = 0\n    model.train()\n    print(f\"Running for epoch {e+1}...\")\n    for images, labels in trainloader:",
        "detail": "001-basic-nn.03-fmnist-dropout",
        "documentation": {}
    },
    {
        "label": "optimizer",
        "kind": 5,
        "importPath": "001-basic-nn.03-fmnist-dropout",
        "description": "001-basic-nn.03-fmnist-dropout",
        "peekOfCode": "optimizer = optim.Adam(model.parameters(), lr=0.003)\nepochs = 30\nsteps = 0\ntrain_losses, test_losses = [], []\nfor e in range(epochs):\n    running_loss = 0\n    model.train()\n    print(f\"Running for epoch {e+1}...\")\n    for images, labels in trainloader:\n        optimizer.zero_grad()",
        "detail": "001-basic-nn.03-fmnist-dropout",
        "documentation": {}
    },
    {
        "label": "epochs",
        "kind": 5,
        "importPath": "001-basic-nn.03-fmnist-dropout",
        "description": "001-basic-nn.03-fmnist-dropout",
        "peekOfCode": "epochs = 30\nsteps = 0\ntrain_losses, test_losses = [], []\nfor e in range(epochs):\n    running_loss = 0\n    model.train()\n    print(f\"Running for epoch {e+1}...\")\n    for images, labels in trainloader:\n        optimizer.zero_grad()\n        log_ps = model(images)",
        "detail": "001-basic-nn.03-fmnist-dropout",
        "documentation": {}
    },
    {
        "label": "steps",
        "kind": 5,
        "importPath": "001-basic-nn.03-fmnist-dropout",
        "description": "001-basic-nn.03-fmnist-dropout",
        "peekOfCode": "steps = 0\ntrain_losses, test_losses = [], []\nfor e in range(epochs):\n    running_loss = 0\n    model.train()\n    print(f\"Running for epoch {e+1}...\")\n    for images, labels in trainloader:\n        optimizer.zero_grad()\n        log_ps = model(images)\n        loss = criterion(log_ps, labels)",
        "detail": "001-basic-nn.03-fmnist-dropout",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "002-transfer-learning.01-transfer-learning",
        "description": "002-transfer-learning.01-transfer-learning",
        "peekOfCode": "model = models.densenet121(pretrained = True)\n# let's define the training and testing dataset\ntraining_transform = transforms.Compose([\n    transforms.RandomRotation(30),\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\ntest_transform = transforms.Compose([",
        "detail": "002-transfer-learning.01-transfer-learning",
        "documentation": {}
    },
    {
        "label": "training_transform",
        "kind": 5,
        "importPath": "002-transfer-learning.01-transfer-learning",
        "description": "002-transfer-learning.01-transfer-learning",
        "peekOfCode": "training_transform = transforms.Compose([\n    transforms.RandomRotation(30),\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\ntest_transform = transforms.Compose([\n    transforms.Resize(255),\n    transforms.CenterCrop(224),",
        "detail": "002-transfer-learning.01-transfer-learning",
        "documentation": {}
    },
    {
        "label": "test_transform",
        "kind": 5,
        "importPath": "002-transfer-learning.01-transfer-learning",
        "description": "002-transfer-learning.01-transfer-learning",
        "peekOfCode": "test_transform = transforms.Compose([\n    transforms.Resize(255),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n# let's now load the images dataset\ndata_dir = \"pet_images\"\ntraining_dataset = datasets.ImageFolder(data_dir+\"/train\", transform=training_transform)\ntesting_dataset = datasets.ImageFolder(data_dir+\"/test\", transform=test_transform)",
        "detail": "002-transfer-learning.01-transfer-learning",
        "documentation": {}
    },
    {
        "label": "data_dir",
        "kind": 5,
        "importPath": "002-transfer-learning.01-transfer-learning",
        "description": "002-transfer-learning.01-transfer-learning",
        "peekOfCode": "data_dir = \"pet_images\"\ntraining_dataset = datasets.ImageFolder(data_dir+\"/train\", transform=training_transform)\ntesting_dataset = datasets.ImageFolder(data_dir+\"/test\", transform=test_transform)\nprint(f\"class to index mapping: {training_dataset.class_to_idx}\")\ntrainloader = torch.utils.data.DataLoader(training_dataset, batch_size=64, shuffle=True)\ntestloader = torch.utils.data.DataLoader(testing_dataset, batch_size=64)\n# now let us modify the last classification layer to replace it with our own new classification layer\n### first let us turn freeze the existing weights so that the pre-existing training will stay as-is\nfor param in model.parameters():\n    param.requires_grad = False",
        "detail": "002-transfer-learning.01-transfer-learning",
        "documentation": {}
    },
    {
        "label": "training_dataset",
        "kind": 5,
        "importPath": "002-transfer-learning.01-transfer-learning",
        "description": "002-transfer-learning.01-transfer-learning",
        "peekOfCode": "training_dataset = datasets.ImageFolder(data_dir+\"/train\", transform=training_transform)\ntesting_dataset = datasets.ImageFolder(data_dir+\"/test\", transform=test_transform)\nprint(f\"class to index mapping: {training_dataset.class_to_idx}\")\ntrainloader = torch.utils.data.DataLoader(training_dataset, batch_size=64, shuffle=True)\ntestloader = torch.utils.data.DataLoader(testing_dataset, batch_size=64)\n# now let us modify the last classification layer to replace it with our own new classification layer\n### first let us turn freeze the existing weights so that the pre-existing training will stay as-is\nfor param in model.parameters():\n    param.requires_grad = False\n### creating our own classification layer",
        "detail": "002-transfer-learning.01-transfer-learning",
        "documentation": {}
    },
    {
        "label": "testing_dataset",
        "kind": 5,
        "importPath": "002-transfer-learning.01-transfer-learning",
        "description": "002-transfer-learning.01-transfer-learning",
        "peekOfCode": "testing_dataset = datasets.ImageFolder(data_dir+\"/test\", transform=test_transform)\nprint(f\"class to index mapping: {training_dataset.class_to_idx}\")\ntrainloader = torch.utils.data.DataLoader(training_dataset, batch_size=64, shuffle=True)\ntestloader = torch.utils.data.DataLoader(testing_dataset, batch_size=64)\n# now let us modify the last classification layer to replace it with our own new classification layer\n### first let us turn freeze the existing weights so that the pre-existing training will stay as-is\nfor param in model.parameters():\n    param.requires_grad = False\n### creating our own classification layer\ncl = nn.Sequential(OrderedDict([",
        "detail": "002-transfer-learning.01-transfer-learning",
        "documentation": {}
    },
    {
        "label": "trainloader",
        "kind": 5,
        "importPath": "002-transfer-learning.01-transfer-learning",
        "description": "002-transfer-learning.01-transfer-learning",
        "peekOfCode": "trainloader = torch.utils.data.DataLoader(training_dataset, batch_size=64, shuffle=True)\ntestloader = torch.utils.data.DataLoader(testing_dataset, batch_size=64)\n# now let us modify the last classification layer to replace it with our own new classification layer\n### first let us turn freeze the existing weights so that the pre-existing training will stay as-is\nfor param in model.parameters():\n    param.requires_grad = False\n### creating our own classification layer\ncl = nn.Sequential(OrderedDict([\n    ('fc1', nn.Linear(1024, 50)),\n    ('relu', nn.ReLU()),",
        "detail": "002-transfer-learning.01-transfer-learning",
        "documentation": {}
    },
    {
        "label": "testloader",
        "kind": 5,
        "importPath": "002-transfer-learning.01-transfer-learning",
        "description": "002-transfer-learning.01-transfer-learning",
        "peekOfCode": "testloader = torch.utils.data.DataLoader(testing_dataset, batch_size=64)\n# now let us modify the last classification layer to replace it with our own new classification layer\n### first let us turn freeze the existing weights so that the pre-existing training will stay as-is\nfor param in model.parameters():\n    param.requires_grad = False\n### creating our own classification layer\ncl = nn.Sequential(OrderedDict([\n    ('fc1', nn.Linear(1024, 50)),\n    ('relu', nn.ReLU()),\n    ('fc2', nn.Linear(50, 2)),",
        "detail": "002-transfer-learning.01-transfer-learning",
        "documentation": {}
    },
    {
        "label": "cl",
        "kind": 5,
        "importPath": "002-transfer-learning.01-transfer-learning",
        "description": "002-transfer-learning.01-transfer-learning",
        "peekOfCode": "cl = nn.Sequential(OrderedDict([\n    ('fc1', nn.Linear(1024, 50)),\n    ('relu', nn.ReLU()),\n    ('fc2', nn.Linear(50, 2)),\n    ('output', nn.LogSoftmax(dim=1))\n]))\nmodel.classifier = cl\n# we will now test the model training performance on cpu vs gpu\nfor device in ['cpu', 'cuda']:\n    criterion = nn.NLLLoss()",
        "detail": "002-transfer-learning.01-transfer-learning",
        "documentation": {}
    },
    {
        "label": "model.classifier",
        "kind": 5,
        "importPath": "002-transfer-learning.01-transfer-learning",
        "description": "002-transfer-learning.01-transfer-learning",
        "peekOfCode": "model.classifier = cl\n# we will now test the model training performance on cpu vs gpu\nfor device in ['cpu', 'cuda']:\n    criterion = nn.NLLLoss()\n    optimizer = optim.Adam(model.classifier.parameters(), lr=0.001) #IMPORTANT: Only train the classification layer parameters\n    model.to(device)\n    for index, (images, labels) in enumerate(trainloader):\n        images, labels = images.to(device), labels.to(device)\n        start = time.time()\n        optimizer.zero_grad()",
        "detail": "002-transfer-learning.01-transfer-learning",
        "documentation": {}
    },
    {
        "label": "Network",
        "kind": 6,
        "importPath": "04-generic-model",
        "description": "04-generic-model",
        "peekOfCode": "class Network(nn.Module):\n    def __init__(self, input_nodes, output_nodes, hidden_nodes_list, drop_p=0.5):        \n        super.__init__()\n        # create the input layer\n        self.input_layer = nn.Linear(input_nodes, hidden_nodes_list[0])\n        # create a list of tuples for the hidden layer nodes, and create hidden layers\n        hidden_layer_sizes = zip(hidden_nodes_list[:-1], hidden_nodes_list[1:])\n        self.hidden_layers = nn.ModuleList([nn.input(h1, h2) for (h1, h2) in hidden_layer_sizes])\n        # create the output layer\n        self.output_layer = nn.Linear(hidden_nodes_list[-1], output_nodes)",
        "detail": "04-generic-model",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "04-generic-model",
        "description": "04-generic-model",
        "peekOfCode": "def train(model, trainloader, testloader, criterion, optimizer, epochs=5, print_every=50):\n    steps = 0\n    for e in range(epochs):\n        runningloss = 0\n        for images, labels in trainloader:\n            steps += 1\n            images = images.view(images.shape[0], -1)\n            optimizer.zero_grad()   # clear the previous grads\n            output = model.forward(images)   # get the output \n            loss = criterion(output, labels)    # get the loss",
        "detail": "04-generic-model",
        "documentation": {}
    },
    {
        "label": "validation",
        "kind": 2,
        "importPath": "04-generic-model",
        "description": "04-generic-model",
        "peekOfCode": "def validation(model, testloader, criterion):\n    accuracy, loss = 0, 0\n    for images, labels in testloader:\n        images = images.view(images.shape[0], -1)\n        output = model.forward(images)\n        # calculate the loss\n        loss += criterion(output, labels)\n        # calculate the accuracy\n        ps = torch.exp(output)  #since output is in log_softmax, take exp to get actual probs\n        equality = (labels.data == ps.max(1)[1])",
        "detail": "04-generic-model",
        "documentation": {}
    }
]