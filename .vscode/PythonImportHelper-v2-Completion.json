[
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "optim",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "optim",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "optim",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "optim",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "datasets",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "datasets",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "datasets",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "datasets",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "models",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "CharTokenizer",
        "importPath": "a_01_tokenizer",
        "description": "a_01_tokenizer",
        "isExtraImport": true,
        "detail": "a_01_tokenizer",
        "documentation": {}
    },
    {
        "label": "CharTokenizer",
        "importPath": "a_01_tokenizer",
        "description": "a_01_tokenizer",
        "isExtraImport": true,
        "detail": "a_01_tokenizer",
        "documentation": {}
    },
    {
        "label": "CharTokenizer",
        "importPath": "a_01_tokenizer",
        "description": "a_01_tokenizer",
        "isExtraImport": true,
        "detail": "a_01_tokenizer",
        "documentation": {}
    },
    {
        "label": "CharTokenizer",
        "importPath": "a_01_tokenizer",
        "description": "a_01_tokenizer",
        "isExtraImport": true,
        "detail": "a_01_tokenizer",
        "documentation": {}
    },
    {
        "label": "CharTokenizer",
        "importPath": "a_01_tokenizer",
        "description": "a_01_tokenizer",
        "isExtraImport": true,
        "detail": "a_01_tokenizer",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "TokenIDsDataset",
        "importPath": "a_02_data_loader",
        "description": "a_02_data_loader",
        "isExtraImport": true,
        "detail": "a_02_data_loader",
        "documentation": {}
    },
    {
        "label": "AttentionHead",
        "importPath": "a_03_attention_block",
        "description": "a_03_attention_block",
        "isExtraImport": true,
        "detail": "a_03_attention_block",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttention",
        "importPath": "a_04_multi_head_attention",
        "description": "a_04_multi_head_attention",
        "isExtraImport": true,
        "detail": "a_04_multi_head_attention",
        "documentation": {}
    },
    {
        "label": "FeedForward",
        "importPath": "a_05_feedforward",
        "description": "a_05_feedforward",
        "isExtraImport": true,
        "detail": "a_05_feedforward",
        "documentation": {}
    },
    {
        "label": "dataset",
        "kind": 5,
        "importPath": "a_002_basic_nn.01-mnist",
        "description": "a_002_basic_nn.01-mnist",
        "peekOfCode": "dataset = \"F_MNIST_data\"\ntrained_dataset_name = \"f_mnist_model.pth\"\n# step 1: load the data, normalize it\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),])\ntrainset =  datasets.MNIST('~/pytorch/'+dataset+'/', download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# step 2: define the model, the criterion (error function), optim (gradient function)\nmodel = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),",
        "detail": "a_002_basic_nn.01-mnist",
        "documentation": {}
    },
    {
        "label": "trained_dataset_name",
        "kind": 5,
        "importPath": "a_002_basic_nn.01-mnist",
        "description": "a_002_basic_nn.01-mnist",
        "peekOfCode": "trained_dataset_name = \"f_mnist_model.pth\"\n# step 1: load the data, normalize it\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),])\ntrainset =  datasets.MNIST('~/pytorch/'+dataset+'/', download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# step 2: define the model, the criterion (error function), optim (gradient function)\nmodel = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 128),",
        "detail": "a_002_basic_nn.01-mnist",
        "documentation": {}
    },
    {
        "label": "transform",
        "kind": 5,
        "importPath": "a_002_basic_nn.01-mnist",
        "description": "a_002_basic_nn.01-mnist",
        "peekOfCode": "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),])\ntrainset =  datasets.MNIST('~/pytorch/'+dataset+'/', download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# step 2: define the model, the criterion (error function), optim (gradient function)\nmodel = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 128),\n    nn.ReLU(),\n    nn.Linear(128, 64),",
        "detail": "a_002_basic_nn.01-mnist",
        "documentation": {}
    },
    {
        "label": "trainset",
        "kind": 5,
        "importPath": "a_002_basic_nn.01-mnist",
        "description": "a_002_basic_nn.01-mnist",
        "peekOfCode": "trainset =  datasets.MNIST('~/pytorch/'+dataset+'/', download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# step 2: define the model, the criterion (error function), optim (gradient function)\nmodel = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 128),\n    nn.ReLU(),\n    nn.Linear(128, 64),\n    nn.ReLU(),",
        "detail": "a_002_basic_nn.01-mnist",
        "documentation": {}
    },
    {
        "label": "trainloader",
        "kind": 5,
        "importPath": "a_002_basic_nn.01-mnist",
        "description": "a_002_basic_nn.01-mnist",
        "peekOfCode": "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# step 2: define the model, the criterion (error function), optim (gradient function)\nmodel = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 128),\n    nn.ReLU(),\n    nn.Linear(128, 64),\n    nn.ReLU(),\n    nn.Linear(64, 10),",
        "detail": "a_002_basic_nn.01-mnist",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "a_002_basic_nn.01-mnist",
        "description": "a_002_basic_nn.01-mnist",
        "peekOfCode": "model = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 128),\n    nn.ReLU(),\n    nn.Linear(128, 64),\n    nn.ReLU(),\n    nn.Linear(64, 10),\n    nn.LogSoftmax(dim=1)\n)",
        "detail": "a_002_basic_nn.01-mnist",
        "documentation": {}
    },
    {
        "label": "criterion",
        "kind": 5,
        "importPath": "a_002_basic_nn.01-mnist",
        "description": "a_002_basic_nn.01-mnist",
        "peekOfCode": "criterion = nn.NLLLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.003)\n# step 3: run the training in loops\nepochs = 5\nfor e in range(epochs):\n    runningloss = 0\n    for images, labels in trainloader:\n        optimizer.zero_grad()   #3a. clear the older grad values, bcs they accumulate\n        images = images.view(images.shape[0], -1)   #3b. flatten the images\n        output = model.forward(images)      #3c. forward pass to get the output",
        "detail": "a_002_basic_nn.01-mnist",
        "documentation": {}
    },
    {
        "label": "optimizer",
        "kind": 5,
        "importPath": "a_002_basic_nn.01-mnist",
        "description": "a_002_basic_nn.01-mnist",
        "peekOfCode": "optimizer = optim.SGD(model.parameters(), lr=0.003)\n# step 3: run the training in loops\nepochs = 5\nfor e in range(epochs):\n    runningloss = 0\n    for images, labels in trainloader:\n        optimizer.zero_grad()   #3a. clear the older grad values, bcs they accumulate\n        images = images.view(images.shape[0], -1)   #3b. flatten the images\n        output = model.forward(images)      #3c. forward pass to get the output\n        loss = criterion(output, labels)    #3d. calculate loss from the output",
        "detail": "a_002_basic_nn.01-mnist",
        "documentation": {}
    },
    {
        "label": "epochs",
        "kind": 5,
        "importPath": "a_002_basic_nn.01-mnist",
        "description": "a_002_basic_nn.01-mnist",
        "peekOfCode": "epochs = 5\nfor e in range(epochs):\n    runningloss = 0\n    for images, labels in trainloader:\n        optimizer.zero_grad()   #3a. clear the older grad values, bcs they accumulate\n        images = images.view(images.shape[0], -1)   #3b. flatten the images\n        output = model.forward(images)      #3c. forward pass to get the output\n        loss = criterion(output, labels)    #3d. calculate loss from the output\n        loss.backward()     #3e. run the backward pass, calc the gradient values\n        optimizer.step()    #3f. apply the gradient descent and update the weights",
        "detail": "a_002_basic_nn.01-mnist",
        "documentation": {}
    },
    {
        "label": "dataset",
        "kind": 5,
        "importPath": "a_002_basic_nn.02-mnist-eval",
        "description": "a_002_basic_nn.02-mnist-eval",
        "peekOfCode": "dataset = \"F_MNIST_data\"\ntrained_dataset_name = \"f_mnist_model.pth\"\n# step 1: load the data, normalize it\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),])\ntrainset =  datasets.MNIST('~/pytorch/'+dataset+'/', download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# step 2: create the loaded model, load the training weights\nloaded_model = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),",
        "detail": "a_002_basic_nn.02-mnist-eval",
        "documentation": {}
    },
    {
        "label": "trained_dataset_name",
        "kind": 5,
        "importPath": "a_002_basic_nn.02-mnist-eval",
        "description": "a_002_basic_nn.02-mnist-eval",
        "peekOfCode": "trained_dataset_name = \"f_mnist_model.pth\"\n# step 1: load the data, normalize it\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),])\ntrainset =  datasets.MNIST('~/pytorch/'+dataset+'/', download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# step 2: create the loaded model, load the training weights\nloaded_model = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 128),",
        "detail": "a_002_basic_nn.02-mnist-eval",
        "documentation": {}
    },
    {
        "label": "transform",
        "kind": 5,
        "importPath": "a_002_basic_nn.02-mnist-eval",
        "description": "a_002_basic_nn.02-mnist-eval",
        "peekOfCode": "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),])\ntrainset =  datasets.MNIST('~/pytorch/'+dataset+'/', download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# step 2: create the loaded model, load the training weights\nloaded_model = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 128),\n    nn.ReLU(),\n    nn.Linear(128, 64),",
        "detail": "a_002_basic_nn.02-mnist-eval",
        "documentation": {}
    },
    {
        "label": "trainset",
        "kind": 5,
        "importPath": "a_002_basic_nn.02-mnist-eval",
        "description": "a_002_basic_nn.02-mnist-eval",
        "peekOfCode": "trainset =  datasets.MNIST('~/pytorch/'+dataset+'/', download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# step 2: create the loaded model, load the training weights\nloaded_model = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 128),\n    nn.ReLU(),\n    nn.Linear(128, 64),\n    nn.ReLU(),",
        "detail": "a_002_basic_nn.02-mnist-eval",
        "documentation": {}
    },
    {
        "label": "trainloader",
        "kind": 5,
        "importPath": "a_002_basic_nn.02-mnist-eval",
        "description": "a_002_basic_nn.02-mnist-eval",
        "peekOfCode": "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# step 2: create the loaded model, load the training weights\nloaded_model = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 128),\n    nn.ReLU(),\n    nn.Linear(128, 64),\n    nn.ReLU(),\n    nn.Linear(64, 10),",
        "detail": "a_002_basic_nn.02-mnist-eval",
        "documentation": {}
    },
    {
        "label": "loaded_model",
        "kind": 5,
        "importPath": "a_002_basic_nn.02-mnist-eval",
        "description": "a_002_basic_nn.02-mnist-eval",
        "peekOfCode": "loaded_model = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 128),\n    nn.ReLU(),\n    nn.Linear(128, 64),\n    nn.ReLU(),\n    nn.Linear(64, 10),\n    nn.LogSoftmax(dim=1)\n)",
        "detail": "a_002_basic_nn.02-mnist-eval",
        "documentation": {}
    },
    {
        "label": "Classifier",
        "kind": 6,
        "importPath": "a_002_basic_nn.03-fmnist-dropout",
        "description": "a_002_basic_nn.03-fmnist-dropout",
        "peekOfCode": "class Classifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.fc4 = nn.Linear(64, 10)\n        # Dropout module with 0.2 drop probability\n        self.dropout = nn.Dropout(p=0.2)\n    def forward(self, x):",
        "detail": "a_002_basic_nn.03-fmnist-dropout",
        "documentation": {}
    },
    {
        "label": "dataset",
        "kind": 5,
        "importPath": "a_002_basic_nn.03-fmnist-dropout",
        "description": "a_002_basic_nn.03-fmnist-dropout",
        "peekOfCode": "dataset = \"F_MNIST_data\"\ntrained_dataset_name = \"f_mnist_dropout_model.pth\"\n# Define a transform to normalize the data\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, ), (0.5, ))])\n# Download and load the training data\ntrainset = datasets.FashionMNIST(f\"~/.pytorch/{dataset}/\", download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# Download and load the test data\ntestset = datasets.FashionMNIST(f\"~/.pytorch/{dataset}/\", download=True, train=False, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)",
        "detail": "a_002_basic_nn.03-fmnist-dropout",
        "documentation": {}
    },
    {
        "label": "trained_dataset_name",
        "kind": 5,
        "importPath": "a_002_basic_nn.03-fmnist-dropout",
        "description": "a_002_basic_nn.03-fmnist-dropout",
        "peekOfCode": "trained_dataset_name = \"f_mnist_dropout_model.pth\"\n# Define a transform to normalize the data\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, ), (0.5, ))])\n# Download and load the training data\ntrainset = datasets.FashionMNIST(f\"~/.pytorch/{dataset}/\", download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# Download and load the test data\ntestset = datasets.FashionMNIST(f\"~/.pytorch/{dataset}/\", download=True, train=False, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\nclass Classifier(nn.Module):",
        "detail": "a_002_basic_nn.03-fmnist-dropout",
        "documentation": {}
    },
    {
        "label": "transform",
        "kind": 5,
        "importPath": "a_002_basic_nn.03-fmnist-dropout",
        "description": "a_002_basic_nn.03-fmnist-dropout",
        "peekOfCode": "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, ), (0.5, ))])\n# Download and load the training data\ntrainset = datasets.FashionMNIST(f\"~/.pytorch/{dataset}/\", download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# Download and load the test data\ntestset = datasets.FashionMNIST(f\"~/.pytorch/{dataset}/\", download=True, train=False, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\nclass Classifier(nn.Module):\n    def __init__(self):\n        super().__init__()",
        "detail": "a_002_basic_nn.03-fmnist-dropout",
        "documentation": {}
    },
    {
        "label": "trainset",
        "kind": 5,
        "importPath": "a_002_basic_nn.03-fmnist-dropout",
        "description": "a_002_basic_nn.03-fmnist-dropout",
        "peekOfCode": "trainset = datasets.FashionMNIST(f\"~/.pytorch/{dataset}/\", download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# Download and load the test data\ntestset = datasets.FashionMNIST(f\"~/.pytorch/{dataset}/\", download=True, train=False, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\nclass Classifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 256)\n        self.fc2 = nn.Linear(256, 128)",
        "detail": "a_002_basic_nn.03-fmnist-dropout",
        "documentation": {}
    },
    {
        "label": "trainloader",
        "kind": 5,
        "importPath": "a_002_basic_nn.03-fmnist-dropout",
        "description": "a_002_basic_nn.03-fmnist-dropout",
        "peekOfCode": "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n# Download and load the test data\ntestset = datasets.FashionMNIST(f\"~/.pytorch/{dataset}/\", download=True, train=False, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\nclass Classifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 64)",
        "detail": "a_002_basic_nn.03-fmnist-dropout",
        "documentation": {}
    },
    {
        "label": "testset",
        "kind": 5,
        "importPath": "a_002_basic_nn.03-fmnist-dropout",
        "description": "a_002_basic_nn.03-fmnist-dropout",
        "peekOfCode": "testset = datasets.FashionMNIST(f\"~/.pytorch/{dataset}/\", download=True, train=False, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\nclass Classifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.fc4 = nn.Linear(64, 10)\n        # Dropout module with 0.2 drop probability",
        "detail": "a_002_basic_nn.03-fmnist-dropout",
        "documentation": {}
    },
    {
        "label": "testloader",
        "kind": 5,
        "importPath": "a_002_basic_nn.03-fmnist-dropout",
        "description": "a_002_basic_nn.03-fmnist-dropout",
        "peekOfCode": "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\nclass Classifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.fc4 = nn.Linear(64, 10)\n        # Dropout module with 0.2 drop probability\n        self.dropout = nn.Dropout(p=0.2)",
        "detail": "a_002_basic_nn.03-fmnist-dropout",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "a_002_basic_nn.03-fmnist-dropout",
        "description": "a_002_basic_nn.03-fmnist-dropout",
        "peekOfCode": "model = Classifier()\ncriterion = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.003)\nepochs = 30\nsteps = 0\ntrain_losses, test_losses = [], []\nfor e in range(epochs):\n    running_loss = 0\n    model.train()\n    print(f\"Running for epoch {e+1}...\")",
        "detail": "a_002_basic_nn.03-fmnist-dropout",
        "documentation": {}
    },
    {
        "label": "criterion",
        "kind": 5,
        "importPath": "a_002_basic_nn.03-fmnist-dropout",
        "description": "a_002_basic_nn.03-fmnist-dropout",
        "peekOfCode": "criterion = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.003)\nepochs = 30\nsteps = 0\ntrain_losses, test_losses = [], []\nfor e in range(epochs):\n    running_loss = 0\n    model.train()\n    print(f\"Running for epoch {e+1}...\")\n    for images, labels in trainloader:",
        "detail": "a_002_basic_nn.03-fmnist-dropout",
        "documentation": {}
    },
    {
        "label": "optimizer",
        "kind": 5,
        "importPath": "a_002_basic_nn.03-fmnist-dropout",
        "description": "a_002_basic_nn.03-fmnist-dropout",
        "peekOfCode": "optimizer = optim.Adam(model.parameters(), lr=0.003)\nepochs = 30\nsteps = 0\ntrain_losses, test_losses = [], []\nfor e in range(epochs):\n    running_loss = 0\n    model.train()\n    print(f\"Running for epoch {e+1}...\")\n    for images, labels in trainloader:\n        optimizer.zero_grad()",
        "detail": "a_002_basic_nn.03-fmnist-dropout",
        "documentation": {}
    },
    {
        "label": "epochs",
        "kind": 5,
        "importPath": "a_002_basic_nn.03-fmnist-dropout",
        "description": "a_002_basic_nn.03-fmnist-dropout",
        "peekOfCode": "epochs = 30\nsteps = 0\ntrain_losses, test_losses = [], []\nfor e in range(epochs):\n    running_loss = 0\n    model.train()\n    print(f\"Running for epoch {e+1}...\")\n    for images, labels in trainloader:\n        optimizer.zero_grad()\n        log_ps = model(images)",
        "detail": "a_002_basic_nn.03-fmnist-dropout",
        "documentation": {}
    },
    {
        "label": "steps",
        "kind": 5,
        "importPath": "a_002_basic_nn.03-fmnist-dropout",
        "description": "a_002_basic_nn.03-fmnist-dropout",
        "peekOfCode": "steps = 0\ntrain_losses, test_losses = [], []\nfor e in range(epochs):\n    running_loss = 0\n    model.train()\n    print(f\"Running for epoch {e+1}...\")\n    for images, labels in trainloader:\n        optimizer.zero_grad()\n        log_ps = model(images)\n        loss = criterion(log_ps, labels)",
        "detail": "a_002_basic_nn.03-fmnist-dropout",
        "documentation": {}
    },
    {
        "label": "Network",
        "kind": 6,
        "importPath": "a_002_basic_nn.04-generic-model",
        "description": "a_002_basic_nn.04-generic-model",
        "peekOfCode": "class Network(nn.Module):\n    def __init__(self, input_nodes, output_nodes, hidden_nodes_list, drop_p=0.5):        \n        super.__init__()\n        # create the input layer\n        self.input_layer = nn.Linear(input_nodes, hidden_nodes_list[0])\n        # create a list of tuples for the hidden layer nodes, and create hidden layers\n        hidden_layer_sizes = zip(hidden_nodes_list[:-1], hidden_nodes_list[1:])\n        self.hidden_layers = nn.ModuleList([nn.input(h1, h2) for (h1, h2) in hidden_layer_sizes])\n        # create the output layer\n        self.output_layer = nn.Linear(hidden_nodes_list[-1], output_nodes)",
        "detail": "a_002_basic_nn.04-generic-model",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "a_002_basic_nn.04-generic-model",
        "description": "a_002_basic_nn.04-generic-model",
        "peekOfCode": "def train(model, trainloader, testloader, criterion, optimizer, epochs=5, print_every=50):\n    steps = 0\n    for e in range(epochs):\n        runningloss = 0\n        for images, labels in trainloader:\n            steps += 1\n            images = images.view(images.shape[0], -1)\n            optimizer.zero_grad()   # clear the previous grads\n            output = model.forward(images)   # get the output \n            loss = criterion(output, labels)    # get the loss",
        "detail": "a_002_basic_nn.04-generic-model",
        "documentation": {}
    },
    {
        "label": "validation",
        "kind": 2,
        "importPath": "a_002_basic_nn.04-generic-model",
        "description": "a_002_basic_nn.04-generic-model",
        "peekOfCode": "def validation(model, testloader, criterion):\n    accuracy, loss = 0, 0\n    for images, labels in testloader:\n        images = images.view(images.shape[0], -1)\n        output = model.forward(images)\n        # calculate the loss\n        loss += criterion(output, labels)\n        # calculate the accuracy\n        ps = torch.exp(output)  #since output is in log_softmax, take exp to get actual probs\n        equality = (labels.data == ps.max(1)[1])",
        "detail": "a_002_basic_nn.04-generic-model",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "a_002_transfer_learning.a_01_transfer_learning",
        "description": "a_002_transfer_learning.a_01_transfer_learning",
        "peekOfCode": "model = models.densenet121(pretrained = True)\n# let's define the training and testing dataset\ntraining_transform = transforms.Compose([\n    transforms.RandomRotation(30),\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\ntest_transform = transforms.Compose([",
        "detail": "a_002_transfer_learning.a_01_transfer_learning",
        "documentation": {}
    },
    {
        "label": "training_transform",
        "kind": 5,
        "importPath": "a_002_transfer_learning.a_01_transfer_learning",
        "description": "a_002_transfer_learning.a_01_transfer_learning",
        "peekOfCode": "training_transform = transforms.Compose([\n    transforms.RandomRotation(30),\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\ntest_transform = transforms.Compose([\n    transforms.Resize(255),\n    transforms.CenterCrop(224),",
        "detail": "a_002_transfer_learning.a_01_transfer_learning",
        "documentation": {}
    },
    {
        "label": "test_transform",
        "kind": 5,
        "importPath": "a_002_transfer_learning.a_01_transfer_learning",
        "description": "a_002_transfer_learning.a_01_transfer_learning",
        "peekOfCode": "test_transform = transforms.Compose([\n    transforms.Resize(255),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n# let's now load the images dataset\ndata_dir = \"../training-data/pet_images\"\ntraining_dataset = datasets.ImageFolder(data_dir+\"/train\", transform=training_transform)\ntesting_dataset = datasets.ImageFolder(data_dir+\"/test\", transform=test_transform)",
        "detail": "a_002_transfer_learning.a_01_transfer_learning",
        "documentation": {}
    },
    {
        "label": "data_dir",
        "kind": 5,
        "importPath": "a_002_transfer_learning.a_01_transfer_learning",
        "description": "a_002_transfer_learning.a_01_transfer_learning",
        "peekOfCode": "data_dir = \"../training-data/pet_images\"\ntraining_dataset = datasets.ImageFolder(data_dir+\"/train\", transform=training_transform)\ntesting_dataset = datasets.ImageFolder(data_dir+\"/test\", transform=test_transform)\nprint(f\"class to index mapping: {training_dataset.class_to_idx}\")\ntrainloader = torch.utils.data.DataLoader(training_dataset, batch_size=64, shuffle=True)\ntestloader = torch.utils.data.DataLoader(testing_dataset, batch_size=64)\n# now let us modify the last classification layer to replace it with our own new classification layer\n### first let us turn freeze the existing weights so that the pre-existing training will stay as-is\nfor param in model.parameters():\n    param.requires_grad = False",
        "detail": "a_002_transfer_learning.a_01_transfer_learning",
        "documentation": {}
    },
    {
        "label": "training_dataset",
        "kind": 5,
        "importPath": "a_002_transfer_learning.a_01_transfer_learning",
        "description": "a_002_transfer_learning.a_01_transfer_learning",
        "peekOfCode": "training_dataset = datasets.ImageFolder(data_dir+\"/train\", transform=training_transform)\ntesting_dataset = datasets.ImageFolder(data_dir+\"/test\", transform=test_transform)\nprint(f\"class to index mapping: {training_dataset.class_to_idx}\")\ntrainloader = torch.utils.data.DataLoader(training_dataset, batch_size=64, shuffle=True)\ntestloader = torch.utils.data.DataLoader(testing_dataset, batch_size=64)\n# now let us modify the last classification layer to replace it with our own new classification layer\n### first let us turn freeze the existing weights so that the pre-existing training will stay as-is\nfor param in model.parameters():\n    param.requires_grad = False\n### creating our own classification layer",
        "detail": "a_002_transfer_learning.a_01_transfer_learning",
        "documentation": {}
    },
    {
        "label": "testing_dataset",
        "kind": 5,
        "importPath": "a_002_transfer_learning.a_01_transfer_learning",
        "description": "a_002_transfer_learning.a_01_transfer_learning",
        "peekOfCode": "testing_dataset = datasets.ImageFolder(data_dir+\"/test\", transform=test_transform)\nprint(f\"class to index mapping: {training_dataset.class_to_idx}\")\ntrainloader = torch.utils.data.DataLoader(training_dataset, batch_size=64, shuffle=True)\ntestloader = torch.utils.data.DataLoader(testing_dataset, batch_size=64)\n# now let us modify the last classification layer to replace it with our own new classification layer\n### first let us turn freeze the existing weights so that the pre-existing training will stay as-is\nfor param in model.parameters():\n    param.requires_grad = False\n### creating our own classification layer\ncl = nn.Sequential(OrderedDict([",
        "detail": "a_002_transfer_learning.a_01_transfer_learning",
        "documentation": {}
    },
    {
        "label": "trainloader",
        "kind": 5,
        "importPath": "a_002_transfer_learning.a_01_transfer_learning",
        "description": "a_002_transfer_learning.a_01_transfer_learning",
        "peekOfCode": "trainloader = torch.utils.data.DataLoader(training_dataset, batch_size=64, shuffle=True)\ntestloader = torch.utils.data.DataLoader(testing_dataset, batch_size=64)\n# now let us modify the last classification layer to replace it with our own new classification layer\n### first let us turn freeze the existing weights so that the pre-existing training will stay as-is\nfor param in model.parameters():\n    param.requires_grad = False\n### creating our own classification layer\ncl = nn.Sequential(OrderedDict([\n    ('fc1', nn.Linear(1024, 50)),\n    ('relu', nn.ReLU()),",
        "detail": "a_002_transfer_learning.a_01_transfer_learning",
        "documentation": {}
    },
    {
        "label": "testloader",
        "kind": 5,
        "importPath": "a_002_transfer_learning.a_01_transfer_learning",
        "description": "a_002_transfer_learning.a_01_transfer_learning",
        "peekOfCode": "testloader = torch.utils.data.DataLoader(testing_dataset, batch_size=64)\n# now let us modify the last classification layer to replace it with our own new classification layer\n### first let us turn freeze the existing weights so that the pre-existing training will stay as-is\nfor param in model.parameters():\n    param.requires_grad = False\n### creating our own classification layer\ncl = nn.Sequential(OrderedDict([\n    ('fc1', nn.Linear(1024, 50)),\n    ('relu', nn.ReLU()),\n    ('fc2', nn.Linear(50, 2)),",
        "detail": "a_002_transfer_learning.a_01_transfer_learning",
        "documentation": {}
    },
    {
        "label": "cl",
        "kind": 5,
        "importPath": "a_002_transfer_learning.a_01_transfer_learning",
        "description": "a_002_transfer_learning.a_01_transfer_learning",
        "peekOfCode": "cl = nn.Sequential(OrderedDict([\n    ('fc1', nn.Linear(1024, 50)),\n    ('relu', nn.ReLU()),\n    ('fc2', nn.Linear(50, 2)),\n    ('output', nn.LogSoftmax(dim=1))\n]))\nmodel.classifier = cl\n# we will now test the model training performance on cpu vs gpu\nfor device in ['cpu', 'cuda']:\n    criterion = nn.NLLLoss()",
        "detail": "a_002_transfer_learning.a_01_transfer_learning",
        "documentation": {}
    },
    {
        "label": "model.classifier",
        "kind": 5,
        "importPath": "a_002_transfer_learning.a_01_transfer_learning",
        "description": "a_002_transfer_learning.a_01_transfer_learning",
        "peekOfCode": "model.classifier = cl\n# we will now test the model training performance on cpu vs gpu\nfor device in ['cpu', 'cuda']:\n    criterion = nn.NLLLoss()\n    optimizer = optim.Adam(model.classifier.parameters(), lr=0.001) #IMPORTANT: Only train the classification layer parameters\n    model.to(device)\n    for index, (images, labels) in enumerate(trainloader):\n        images, labels = images.to(device), labels.to(device)\n        start = time.time()\n        optimizer.zero_grad()",
        "detail": "a_002_transfer_learning.a_01_transfer_learning",
        "documentation": {}
    },
    {
        "label": "CharTokenizer",
        "kind": 6,
        "importPath": "a_003_transformers_basics.a_01_tokenizer",
        "description": "a_003_transformers_basics.a_01_tokenizer",
        "peekOfCode": "class CharTokenizer:\n    def __init__(self, vocab):\n        self.char_to_tokenid = { char:index  for index, char in enumerate(vocab)}\n        self.tokenid_to_char = { index:char  for index, char in enumerate(vocab)}\n    @staticmethod\n    def trainFromText(text):\n        vocab = sorted(list(set(text)))\n        return CharTokenizer(vocab)\n    def encode(self, text):\n        return torch.tensor([self.char_to_tokenid[ch] for ch in text])",
        "detail": "a_003_transformers_basics.a_01_tokenizer",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_01_tokenizer",
        "description": "a_003_transformers_basics.a_01_tokenizer",
        "peekOfCode": "text = Path('../training-data/tiny-shakespeare.txt').read_text()\nprint(text[:1000])\nct = CharTokenizer.trainFromText(text)\na = \"hello world!\"\nencoded = ct.encode(a)\ndecoded = ct.decode(encoded)\nprint(encoded)\nprint(decoded)",
        "detail": "a_003_transformers_basics.a_01_tokenizer",
        "documentation": {}
    },
    {
        "label": "ct",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_01_tokenizer",
        "description": "a_003_transformers_basics.a_01_tokenizer",
        "peekOfCode": "ct = CharTokenizer.trainFromText(text)\na = \"hello world!\"\nencoded = ct.encode(a)\ndecoded = ct.decode(encoded)\nprint(encoded)\nprint(decoded)",
        "detail": "a_003_transformers_basics.a_01_tokenizer",
        "documentation": {}
    },
    {
        "label": "a",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_01_tokenizer",
        "description": "a_003_transformers_basics.a_01_tokenizer",
        "peekOfCode": "a = \"hello world!\"\nencoded = ct.encode(a)\ndecoded = ct.decode(encoded)\nprint(encoded)\nprint(decoded)",
        "detail": "a_003_transformers_basics.a_01_tokenizer",
        "documentation": {}
    },
    {
        "label": "encoded",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_01_tokenizer",
        "description": "a_003_transformers_basics.a_01_tokenizer",
        "peekOfCode": "encoded = ct.encode(a)\ndecoded = ct.decode(encoded)\nprint(encoded)\nprint(decoded)",
        "detail": "a_003_transformers_basics.a_01_tokenizer",
        "documentation": {}
    },
    {
        "label": "decoded",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_01_tokenizer",
        "description": "a_003_transformers_basics.a_01_tokenizer",
        "peekOfCode": "decoded = ct.decode(encoded)\nprint(encoded)\nprint(decoded)",
        "detail": "a_003_transformers_basics.a_01_tokenizer",
        "documentation": {}
    },
    {
        "label": "TokenIDsDataset",
        "kind": 6,
        "importPath": "a_003_transformers_basics.a_02_data_loader",
        "description": "a_003_transformers_basics.a_02_data_loader",
        "peekOfCode": "class TokenIDsDataset(Dataset):\n    def __init__(self, data, block_size):\n        self.data = data\n        self.block_size = block_size\n    def __len__(self):\n        # We can only start the sequences till the index where\n        # we would have enough tokens available to generate the\n        # output sequence as well\n        return len(self.data) - self.block_size\n    def __getitem__(self, pos):",
        "detail": "a_003_transformers_basics.a_02_data_loader",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_02_data_loader",
        "description": "a_003_transformers_basics.a_02_data_loader",
        "peekOfCode": "text = Path('../training-data/tiny-shakespeare.txt').read_text()\n# 1. Creating a tokenizer out of our text data\nct = CharTokenizer.trainFromText(text)\n# 2. Creating the dataset for training by first encoding the data into tokens\nencoded = ct.encode(text)\n# 3. Create the dataset class\nblock_size = 64\ndataset = TokenIDsDataset(data=encoded, block_size=block_size)  \n# 4. Get the first item from the dataset and decode it\nx, y = dataset[0]",
        "detail": "a_003_transformers_basics.a_02_data_loader",
        "documentation": {}
    },
    {
        "label": "ct",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_02_data_loader",
        "description": "a_003_transformers_basics.a_02_data_loader",
        "peekOfCode": "ct = CharTokenizer.trainFromText(text)\n# 2. Creating the dataset for training by first encoding the data into tokens\nencoded = ct.encode(text)\n# 3. Create the dataset class\nblock_size = 64\ndataset = TokenIDsDataset(data=encoded, block_size=block_size)  \n# 4. Get the first item from the dataset and decode it\nx, y = dataset[0]\nprint(\"X: \", ct.decode(x))\nprint(\"Y: \", ct.decode(y))",
        "detail": "a_003_transformers_basics.a_02_data_loader",
        "documentation": {}
    },
    {
        "label": "encoded",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_02_data_loader",
        "description": "a_003_transformers_basics.a_02_data_loader",
        "peekOfCode": "encoded = ct.encode(text)\n# 3. Create the dataset class\nblock_size = 64\ndataset = TokenIDsDataset(data=encoded, block_size=block_size)  \n# 4. Get the first item from the dataset and decode it\nx, y = dataset[0]\nprint(\"X: \", ct.decode(x))\nprint(\"Y: \", ct.decode(y))",
        "detail": "a_003_transformers_basics.a_02_data_loader",
        "documentation": {}
    },
    {
        "label": "block_size",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_02_data_loader",
        "description": "a_003_transformers_basics.a_02_data_loader",
        "peekOfCode": "block_size = 64\ndataset = TokenIDsDataset(data=encoded, block_size=block_size)  \n# 4. Get the first item from the dataset and decode it\nx, y = dataset[0]\nprint(\"X: \", ct.decode(x))\nprint(\"Y: \", ct.decode(y))",
        "detail": "a_003_transformers_basics.a_02_data_loader",
        "documentation": {}
    },
    {
        "label": "dataset",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_02_data_loader",
        "description": "a_003_transformers_basics.a_02_data_loader",
        "peekOfCode": "dataset = TokenIDsDataset(data=encoded, block_size=block_size)  \n# 4. Get the first item from the dataset and decode it\nx, y = dataset[0]\nprint(\"X: \", ct.decode(x))\nprint(\"Y: \", ct.decode(y))",
        "detail": "a_003_transformers_basics.a_02_data_loader",
        "documentation": {}
    },
    {
        "label": "AttentionHead",
        "kind": 6,
        "importPath": "a_003_transformers_basics.a_03_attention_block",
        "description": "a_003_transformers_basics.a_03_attention_block",
        "peekOfCode": "class AttentionHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.WQ = nn.Linear(config['embedding_dim'], config['head_size'], config['use_bias'])\n        self.WK = nn.Linear(config['embedding_dim'], config['head_size'], config['use_bias'])\n        self.WV = nn.Linear(config['embedding_dim'], config['head_size'], config['use_bias'])\n        self.dropout = nn.Dropout(config[\"dropout_rate\"])\n        causal_attention_mask = torch.tril(torch.ones(config['context_size'], config['context_size']))\n        self.register_buffer(\"causal_attention_mask\", causal_attention_mask)\n    def forward(self, input):   # (B, C, embedding_dim)",
        "detail": "a_003_transformers_basics.a_03_attention_block",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_03_attention_block",
        "description": "a_003_transformers_basics.a_03_attention_block",
        "peekOfCode": "text = Path('../training-data/tiny-shakespeare.txt').read_text()\ntokenizer = CharTokenizer.trainFromText(text)\nprint(tokenizer.encode(\"hello world\"))\nprint(tokenizer.decode(tokenizer.encode(\"hello world\")))\n# config = {\n#     \"vocabulary_size\": tokenizer.vocabSize(),   #size of vocab / # of unique token IDs that it supports\n#     \"context_size\": 256,    #lentgh of the context window / max number of tokens the model can see at once\n#     \"embedding_dim\": 768,   #length of the embedding vector, each token will be converted to an embedding vector of this length\n#     \"heads_num\": 12,        #num of attention heads we'll have in the model, each of them will process the input independently\n#     \"layers_num\": 10,       #num of layers / transformers blocks in the model",
        "detail": "a_003_transformers_basics.a_03_attention_block",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_03_attention_block",
        "description": "a_003_transformers_basics.a_03_attention_block",
        "peekOfCode": "tokenizer = CharTokenizer.trainFromText(text)\nprint(tokenizer.encode(\"hello world\"))\nprint(tokenizer.decode(tokenizer.encode(\"hello world\")))\n# config = {\n#     \"vocabulary_size\": tokenizer.vocabSize(),   #size of vocab / # of unique token IDs that it supports\n#     \"context_size\": 256,    #lentgh of the context window / max number of tokens the model can see at once\n#     \"embedding_dim\": 768,   #length of the embedding vector, each token will be converted to an embedding vector of this length\n#     \"heads_num\": 12,        #num of attention heads we'll have in the model, each of them will process the input independently\n#     \"layers_num\": 10,       #num of layers / transformers blocks in the model\n#     \"dropout_rate\": 0.1,    #probability of dropping out nodes on random",
        "detail": "a_003_transformers_basics.a_03_attention_block",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_03_attention_block",
        "description": "a_003_transformers_basics.a_03_attention_block",
        "peekOfCode": "config = {\n    \"vocabulary_size\": tokenizer.vocabSize(),   #size of vocab / # of unique token IDs that it supports\n    \"context_size\": 32,    #lentgh of the context window / max number of tokens the model can see at once\n    \"embedding_dim\": 64,   #length of the embedding vector, each token will be converted to an embedding vector of this length\n    \"heads_num\": 4,        #num of attention heads we'll have in the model, each of them will process the input independently\n    \"layers_num\": 2,       #num of layers / transformers blocks in the model\n    \"dropout_rate\": 0.1,    #probability of dropping out nodes on random\n    \"use_bias\": False,      #whether the linear transformations should include bias terms\n}\n# we need to set the size of output form each head correctly",
        "detail": "a_003_transformers_basics.a_03_attention_block",
        "documentation": {}
    },
    {
        "label": "config[\"head_size\"]",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_03_attention_block",
        "description": "a_003_transformers_basics.a_03_attention_block",
        "peekOfCode": "config[\"head_size\"] = config[\"embedding_dim\"] // config[\"heads_num\"]\n# 1. let's first implement a single attention head\nclass AttentionHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.WQ = nn.Linear(config['embedding_dim'], config['head_size'], config['use_bias'])\n        self.WK = nn.Linear(config['embedding_dim'], config['head_size'], config['use_bias'])\n        self.WV = nn.Linear(config['embedding_dim'], config['head_size'], config['use_bias'])\n        self.dropout = nn.Dropout(config[\"dropout_rate\"])\n        causal_attention_mask = torch.tril(torch.ones(config['context_size'], config['context_size']))",
        "detail": "a_003_transformers_basics.a_03_attention_block",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_03_attention_block",
        "description": "a_003_transformers_basics.a_03_attention_block",
        "peekOfCode": "batch_size = 8\ncontext_size = 256\nembedding_dim = 768\n\"\"\"\ninp = torch.rand(1, config['context_size'], config['embedding_dim'])\nah = AttentionHead(config)\noutput = ah(inp)\nprint(f\"Output size of a single attention head: {output.shape}\")",
        "detail": "a_003_transformers_basics.a_03_attention_block",
        "documentation": {}
    },
    {
        "label": "context_size",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_03_attention_block",
        "description": "a_003_transformers_basics.a_03_attention_block",
        "peekOfCode": "context_size = 256\nembedding_dim = 768\n\"\"\"\ninp = torch.rand(1, config['context_size'], config['embedding_dim'])\nah = AttentionHead(config)\noutput = ah(inp)\nprint(f\"Output size of a single attention head: {output.shape}\")",
        "detail": "a_003_transformers_basics.a_03_attention_block",
        "documentation": {}
    },
    {
        "label": "embedding_dim",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_03_attention_block",
        "description": "a_003_transformers_basics.a_03_attention_block",
        "peekOfCode": "embedding_dim = 768\n\"\"\"\ninp = torch.rand(1, config['context_size'], config['embedding_dim'])\nah = AttentionHead(config)\noutput = ah(inp)\nprint(f\"Output size of a single attention head: {output.shape}\")",
        "detail": "a_003_transformers_basics.a_03_attention_block",
        "documentation": {}
    },
    {
        "label": "inp",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_03_attention_block",
        "description": "a_003_transformers_basics.a_03_attention_block",
        "peekOfCode": "inp = torch.rand(1, config['context_size'], config['embedding_dim'])\nah = AttentionHead(config)\noutput = ah(inp)\nprint(f\"Output size of a single attention head: {output.shape}\")",
        "detail": "a_003_transformers_basics.a_03_attention_block",
        "documentation": {}
    },
    {
        "label": "ah",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_03_attention_block",
        "description": "a_003_transformers_basics.a_03_attention_block",
        "peekOfCode": "ah = AttentionHead(config)\noutput = ah(inp)\nprint(f\"Output size of a single attention head: {output.shape}\")",
        "detail": "a_003_transformers_basics.a_03_attention_block",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_03_attention_block",
        "description": "a_003_transformers_basics.a_03_attention_block",
        "peekOfCode": "output = ah(inp)\nprint(f\"Output size of a single attention head: {output.shape}\")",
        "detail": "a_003_transformers_basics.a_03_attention_block",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttention",
        "kind": 6,
        "importPath": "a_003_transformers_basics.a_04_multi_head_attention",
        "description": "a_003_transformers_basics.a_04_multi_head_attention",
        "peekOfCode": "class MultiHeadAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        # create a list of attentionheads, each initialized with the config param, length = config[\"heads_num\"]\n        heads_list = [AttentionHead(config) for _ in range(config[\"heads_num\"])]\n        # store that created list in ModuleList function\n        self.heads = nn.ModuleList(heads_list)\n        # create a linear layer with size embedding_dim * embedding_dim\n        self.linear = nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"])\n        # create a dropout layer with rate = config[\"dropout_rate\"], store it in self.dropout",
        "detail": "a_003_transformers_basics.a_04_multi_head_attention",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_04_multi_head_attention",
        "description": "a_003_transformers_basics.a_04_multi_head_attention",
        "peekOfCode": "text = Path('../training-data/tiny-shakespeare.txt').read_text()\ntokenizer = CharTokenizer.trainFromText(text)\nconfig = {\n    \"vocabulary_size\": tokenizer.vocabSize(),   #size of vocab / # of unique token IDs that it supports\n    \"context_size\": 32,    #lentgh of the context window / max number of tokens the model can see at once\n    \"embedding_dim\": 64,   #length of the embedding vector, each token will be converted to an embedding vector of this length\n    \"heads_num\": 4,        #num of attention heads we'll have in the model, each of them will process the input independently\n    \"layers_num\": 2,       #num of layers / transformers blocks in the model\n    \"dropout_rate\": 0.1,    #probability of dropping out nodes on random\n    \"use_bias\": False,      #whether the linear transformations should include bias terms",
        "detail": "a_003_transformers_basics.a_04_multi_head_attention",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_04_multi_head_attention",
        "description": "a_003_transformers_basics.a_04_multi_head_attention",
        "peekOfCode": "tokenizer = CharTokenizer.trainFromText(text)\nconfig = {\n    \"vocabulary_size\": tokenizer.vocabSize(),   #size of vocab / # of unique token IDs that it supports\n    \"context_size\": 32,    #lentgh of the context window / max number of tokens the model can see at once\n    \"embedding_dim\": 64,   #length of the embedding vector, each token will be converted to an embedding vector of this length\n    \"heads_num\": 4,        #num of attention heads we'll have in the model, each of them will process the input independently\n    \"layers_num\": 2,       #num of layers / transformers blocks in the model\n    \"dropout_rate\": 0.1,    #probability of dropping out nodes on random\n    \"use_bias\": False,      #whether the linear transformations should include bias terms\n}",
        "detail": "a_003_transformers_basics.a_04_multi_head_attention",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_04_multi_head_attention",
        "description": "a_003_transformers_basics.a_04_multi_head_attention",
        "peekOfCode": "config = {\n    \"vocabulary_size\": tokenizer.vocabSize(),   #size of vocab / # of unique token IDs that it supports\n    \"context_size\": 32,    #lentgh of the context window / max number of tokens the model can see at once\n    \"embedding_dim\": 64,   #length of the embedding vector, each token will be converted to an embedding vector of this length\n    \"heads_num\": 4,        #num of attention heads we'll have in the model, each of them will process the input independently\n    \"layers_num\": 2,       #num of layers / transformers blocks in the model\n    \"dropout_rate\": 0.1,    #probability of dropping out nodes on random\n    \"use_bias\": False,      #whether the linear transformations should include bias terms\n}\nconfig[\"head_size\"] = config[\"embedding_dim\"] // config[\"heads_num\"]",
        "detail": "a_003_transformers_basics.a_04_multi_head_attention",
        "documentation": {}
    },
    {
        "label": "config[\"head_size\"]",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_04_multi_head_attention",
        "description": "a_003_transformers_basics.a_04_multi_head_attention",
        "peekOfCode": "config[\"head_size\"] = config[\"embedding_dim\"] // config[\"heads_num\"]\n####### Runner code #######\ninput = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])\nmha = MultiHeadAttention(config)\noutput = mha(input)\nprint(f\"Output size of the multiple attention heads: {output.shape}\")",
        "detail": "a_003_transformers_basics.a_04_multi_head_attention",
        "documentation": {}
    },
    {
        "label": "input",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_04_multi_head_attention",
        "description": "a_003_transformers_basics.a_04_multi_head_attention",
        "peekOfCode": "input = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])\nmha = MultiHeadAttention(config)\noutput = mha(input)\nprint(f\"Output size of the multiple attention heads: {output.shape}\")",
        "detail": "a_003_transformers_basics.a_04_multi_head_attention",
        "documentation": {}
    },
    {
        "label": "mha",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_04_multi_head_attention",
        "description": "a_003_transformers_basics.a_04_multi_head_attention",
        "peekOfCode": "mha = MultiHeadAttention(config)\noutput = mha(input)\nprint(f\"Output size of the multiple attention heads: {output.shape}\")",
        "detail": "a_003_transformers_basics.a_04_multi_head_attention",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_04_multi_head_attention",
        "description": "a_003_transformers_basics.a_04_multi_head_attention",
        "peekOfCode": "output = mha(input)\nprint(f\"Output size of the multiple attention heads: {output.shape}\")",
        "detail": "a_003_transformers_basics.a_04_multi_head_attention",
        "documentation": {}
    },
    {
        "label": "FeedForward",
        "kind": 6,
        "importPath": "a_003_transformers_basics.a_05_feedforward",
        "description": "a_003_transformers_basics.a_05_feedforward",
        "peekOfCode": "class FeedForward(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        # create a sequential nn with the following layers\n        # 1. expand the input size to 4 times\n        # 2. pass it through gelu\n        # 3. contract the output size back to original\n        # 4. pass it through dropout layer\n        self.linear_layers = nn.Sequential(\n            nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"]*4),",
        "detail": "a_003_transformers_basics.a_05_feedforward",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_05_feedforward",
        "description": "a_003_transformers_basics.a_05_feedforward",
        "peekOfCode": "text = Path('../training-data/tiny-shakespeare.txt').read_text()\ntokenizer = CharTokenizer.trainFromText(text)\nconfig = {\n    \"vocabulary_size\": tokenizer.vocabSize(),   #size of vocab / # of unique token IDs that it supports\n    \"context_size\": 32,    #lentgh of the context window / max number of tokens the model can see at once\n    \"embedding_dim\": 64,   #length of the embedding vector, each token will be converted to an embedding vector of this length\n    \"heads_num\": 4,        #num of attention heads we'll have in the model, each of them will process the input independently\n    \"layers_num\": 2,       #num of layers / transformers blocks in the model\n    \"dropout_rate\": 0.1,    #probability of dropping out nodes on random\n    \"use_bias\": False,      #whether the linear transformations should include bias terms",
        "detail": "a_003_transformers_basics.a_05_feedforward",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_05_feedforward",
        "description": "a_003_transformers_basics.a_05_feedforward",
        "peekOfCode": "tokenizer = CharTokenizer.trainFromText(text)\nconfig = {\n    \"vocabulary_size\": tokenizer.vocabSize(),   #size of vocab / # of unique token IDs that it supports\n    \"context_size\": 32,    #lentgh of the context window / max number of tokens the model can see at once\n    \"embedding_dim\": 64,   #length of the embedding vector, each token will be converted to an embedding vector of this length\n    \"heads_num\": 4,        #num of attention heads we'll have in the model, each of them will process the input independently\n    \"layers_num\": 2,       #num of layers / transformers blocks in the model\n    \"dropout_rate\": 0.1,    #probability of dropping out nodes on random\n    \"use_bias\": False,      #whether the linear transformations should include bias terms\n}",
        "detail": "a_003_transformers_basics.a_05_feedforward",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_05_feedforward",
        "description": "a_003_transformers_basics.a_05_feedforward",
        "peekOfCode": "config = {\n    \"vocabulary_size\": tokenizer.vocabSize(),   #size of vocab / # of unique token IDs that it supports\n    \"context_size\": 32,    #lentgh of the context window / max number of tokens the model can see at once\n    \"embedding_dim\": 64,   #length of the embedding vector, each token will be converted to an embedding vector of this length\n    \"heads_num\": 4,        #num of attention heads we'll have in the model, each of them will process the input independently\n    \"layers_num\": 2,       #num of layers / transformers blocks in the model\n    \"dropout_rate\": 0.1,    #probability of dropping out nodes on random\n    \"use_bias\": False,      #whether the linear transformations should include bias terms\n}\nconfig[\"head_size\"] = config[\"embedding_dim\"] // config[\"heads_num\"]",
        "detail": "a_003_transformers_basics.a_05_feedforward",
        "documentation": {}
    },
    {
        "label": "config[\"head_size\"]",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_05_feedforward",
        "description": "a_003_transformers_basics.a_05_feedforward",
        "peekOfCode": "config[\"head_size\"] = config[\"embedding_dim\"] // config[\"heads_num\"]\n####### Runner code #######\nff = FeedForward(config)\ninp = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])\noutput = ff(inp)\nprint(output.shape)",
        "detail": "a_003_transformers_basics.a_05_feedforward",
        "documentation": {}
    },
    {
        "label": "ff",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_05_feedforward",
        "description": "a_003_transformers_basics.a_05_feedforward",
        "peekOfCode": "ff = FeedForward(config)\ninp = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])\noutput = ff(inp)\nprint(output.shape)",
        "detail": "a_003_transformers_basics.a_05_feedforward",
        "documentation": {}
    },
    {
        "label": "inp",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_05_feedforward",
        "description": "a_003_transformers_basics.a_05_feedforward",
        "peekOfCode": "inp = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])\noutput = ff(inp)\nprint(output.shape)",
        "detail": "a_003_transformers_basics.a_05_feedforward",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_05_feedforward",
        "description": "a_003_transformers_basics.a_05_feedforward",
        "peekOfCode": "output = ff(inp)\nprint(output.shape)",
        "detail": "a_003_transformers_basics.a_05_feedforward",
        "documentation": {}
    },
    {
        "label": "TransformerBlock",
        "kind": 6,
        "importPath": "a_003_transformers_basics.a_06_transformer_block",
        "description": "a_003_transformers_basics.a_06_transformer_block",
        "peekOfCode": "class TransformerBlock(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        # set the following elements in the self\n        # - multi head attention block\n        # - normalisation layer 1\n        # - feedforward layer\n        # - normalisation layer 2\n        self.multi_head = MultiHeadAttention(config)\n        self.layer_norm_1 = nn.LayerNorm(config[\"embedding_dim\"])",
        "detail": "a_003_transformers_basics.a_06_transformer_block",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_06_transformer_block",
        "description": "a_003_transformers_basics.a_06_transformer_block",
        "peekOfCode": "text = Path('../training-data/tiny-shakespeare.txt').read_text()\ntokenizer = CharTokenizer.trainFromText(text)\nconfig = {\n    \"vocabulary_size\": tokenizer.vocabSize(),   #size of vocab / # of unique token IDs that it supports\n    \"context_size\": 32,    #lentgh of the context window / max number of tokens the model can see at once\n    \"embedding_dim\": 64,   #length of the embedding vector, each token will be converted to an embedding vector of this length\n    \"heads_num\": 4,        #num of attention heads we'll have in the model, each of them will process the input independently\n    \"layers_num\": 2,       #num of layers / transformers blocks in the model\n    \"dropout_rate\": 0.1,    #probability of dropping out nodes on random\n    \"use_bias\": False,      #whether the linear transformations should include bias terms",
        "detail": "a_003_transformers_basics.a_06_transformer_block",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_06_transformer_block",
        "description": "a_003_transformers_basics.a_06_transformer_block",
        "peekOfCode": "tokenizer = CharTokenizer.trainFromText(text)\nconfig = {\n    \"vocabulary_size\": tokenizer.vocabSize(),   #size of vocab / # of unique token IDs that it supports\n    \"context_size\": 32,    #lentgh of the context window / max number of tokens the model can see at once\n    \"embedding_dim\": 64,   #length of the embedding vector, each token will be converted to an embedding vector of this length\n    \"heads_num\": 4,        #num of attention heads we'll have in the model, each of them will process the input independently\n    \"layers_num\": 2,       #num of layers / transformers blocks in the model\n    \"dropout_rate\": 0.1,    #probability of dropping out nodes on random\n    \"use_bias\": False,      #whether the linear transformations should include bias terms\n}",
        "detail": "a_003_transformers_basics.a_06_transformer_block",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_06_transformer_block",
        "description": "a_003_transformers_basics.a_06_transformer_block",
        "peekOfCode": "config = {\n    \"vocabulary_size\": tokenizer.vocabSize(),   #size of vocab / # of unique token IDs that it supports\n    \"context_size\": 32,    #lentgh of the context window / max number of tokens the model can see at once\n    \"embedding_dim\": 64,   #length of the embedding vector, each token will be converted to an embedding vector of this length\n    \"heads_num\": 4,        #num of attention heads we'll have in the model, each of them will process the input independently\n    \"layers_num\": 2,       #num of layers / transformers blocks in the model\n    \"dropout_rate\": 0.1,    #probability of dropping out nodes on random\n    \"use_bias\": False,      #whether the linear transformations should include bias terms\n}\nconfig[\"head_size\"] = config[\"embedding_dim\"] // config[\"heads_num\"]",
        "detail": "a_003_transformers_basics.a_06_transformer_block",
        "documentation": {}
    },
    {
        "label": "config[\"head_size\"]",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_06_transformer_block",
        "description": "a_003_transformers_basics.a_06_transformer_block",
        "peekOfCode": "config[\"head_size\"] = config[\"embedding_dim\"] // config[\"heads_num\"]\n####### Runner code ########\ntb = TransformerBlock(config)\ninput = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])\noutput = tb(input)\nprint(output.shape)",
        "detail": "a_003_transformers_basics.a_06_transformer_block",
        "documentation": {}
    },
    {
        "label": "tb",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_06_transformer_block",
        "description": "a_003_transformers_basics.a_06_transformer_block",
        "peekOfCode": "tb = TransformerBlock(config)\ninput = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])\noutput = tb(input)\nprint(output.shape)",
        "detail": "a_003_transformers_basics.a_06_transformer_block",
        "documentation": {}
    },
    {
        "label": "input",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_06_transformer_block",
        "description": "a_003_transformers_basics.a_06_transformer_block",
        "peekOfCode": "input = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])\noutput = tb(input)\nprint(output.shape)",
        "detail": "a_003_transformers_basics.a_06_transformer_block",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "a_003_transformers_basics.a_06_transformer_block",
        "description": "a_003_transformers_basics.a_06_transformer_block",
        "peekOfCode": "output = tb(input)\nprint(output.shape)",
        "detail": "a_003_transformers_basics.a_06_transformer_block",
        "documentation": {}
    }
]